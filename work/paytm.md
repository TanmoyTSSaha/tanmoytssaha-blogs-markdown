---
id: paytm-data-engineer
company: "Paytm"
role: "Data Engineer"
location: "Noida, IN"
type: "Full-time"
start_date: "2025-02-20"
end_date: "Present"
is_current: true
company_url: "https://paytm.com/"
logo_url: "https://cdn.iconscout.com/icon/free/png-512/free-paytm-icon-svg-download-png-226448.png"
technologies:
  - Python
  - Scala
  - Spark
  - AWS
  - Hive
  - Hadoop
  - Iceberg
  - ReactJS
  - TailwindCSS
  - Git
  - BitBucket
  - Docker
  - Azkaban
description: "Worked on developing near real-time data pipeline along with optimising existing projects"
---

## Overview

At SpiceJet, I worked as a data engineer on there big data infrastructure by optimising existing project and created a new nearly real-time data pipeline for analytics purpose.

---

## Key Responsibilities

- Reduced S3 storage by 45 percent by implementing and optimizing Life Cycle Policies (LCP), including removal of obsolete data and non-functional policies through strategic auditing and analysis of usage patterns.
- Boosted Spark job performance by narrowing data scans and eliminating redundant transformations, resulting in faster execution and reduced compute costs.
- Developed an AI MCP Tool that automated the development and deployment of real-time data pipelines, integrated with Cursor AI Agent â€” reducing deployment time from 2 hours of manual efforts to just 30 minutes of AI base automation.
- Refactored the batch data pipeline codebase by removing flat-file dependencies and centralizing metadata storage in a database, making it accessible through APIs for easier integration and maintenance.
- Improved internal monitoring dashboards by redesigning the UI and integrating additional metrics and alerting capabilities for better observability.
- Worked on migrating streaming pipeline on organisational level migration which include migrating many microservices supporting the streaming pipeline.
- Worked on optimizing and adding new BAU reqirements on the existing batch and streaming pipeline. 
- Participated in oncalls and solved major bugs.

---

## Key Achievements

- Reduced and optimized the S3 storage which directly effected in reducing the cost.
- Participated in organizational level migration and migrated microservices supporting streaming pipeline.
- Developed AI based MCP server which onboarding and deployment process of BAU tasks in streaming pipeline easy.
- Improved the UI of existing portal and added monitoring metrices using grafana directly into the portal.
- Participated in Oncalls and fixed some major bugs during oncall and supported the smooth running of jobs.

---

## Impact

- Solved major bugs and supported the smooth running of jobs.
- Improved onboarding of BAU tasks from 2 Days to few hours.
- Impacted in reducing cost by optimizing S3 storage.

---

## Technologies Deep Dive

### Data Engineering
- Scala
- Spark
- Hive
- Hadoop
- Iceberg
- Azkaban
- Zeppelin
- Kafka

### Fullstack
- Python
- ReactJS
- NodeJS
- ExpressJS
- HTML
- CSS
- TailwindCSS
- JavaScript


### Infrastructure
- AWS (S3, ECR, EKS, EC2, EMR, Lambda, RDS)
- Docker
- Kubernetes
- Jenknins
- ArgoCD
- Linux`

### Observability
- Alerting mechanism using emails and slack
- Prometheus & Grafana
