---
id: paytm-data-engineer
company: "Paytm"
role: "Data Engineer"
location: "Noida, IN"
type: "Full-time"
start_date: "2025-02-20"
end_date: "Present"
is_current: true
company_url: "https://paytm.com/"
logo_url: "https://cdn.iconscout.com/icon/free/png-512/free-paytm-icon-svg-download-png-226448.png"
technologies:
  - Python
  - Scala
  - Spark
  - AWS
  - Hive
  - Hadoop
  - Iceberg
  - ReactJS
  - TailwindCSS
  - Git
  - BitBucket
  - Docker
  - Azkaban
description: "Worked on developing near real-time data pipeline along with optimising existing projects"
---

## Overview

At Paytm, I work on big data infrastructure — optimizing Spark jobs and S3 storage, migrating streaming pipelines, and building internal tooling. I also built an AI-powered MCP tool that sped up pipeline deployments, revamped monitoring dashboards, and handle on-call rotations to keep things running smoothly.

---

## Key Responsibilities

- Achieved 45% reduction in S3 storage costs by implementing optimized Lifecycle Policies (LCP), auditing usage patterns, and eliminating obsolete data and redundant policies.
- Optimized Spark job performance through partition pruning, predicate pushdown, and elimination of redundant transformations—reducing execution time and compute costs significantly.
- Built an AI-powered MCP Tool integrated with Cursor AI Agent to automate real-time data pipeline development and deployment, cutting deployment time from 2 hours to 30 minutes (75% reduction).
- Modernized batch pipeline architecture by replacing flat-file dependencies with centralized database-backed metadata storage, exposed via RESTful APIs for seamless integration.
- Redesigned internal monitoring dashboards with enhanced UI/UX and integrated Grafana metrics for improved observability.
- Led migration of streaming pipelines and associated microservices as part of an organization-wide infrastructure migration initiative.
- Delivered ongoing BAU enhancements and optimizations across batch and streaming pipelines to meet evolving business requirements.
- Managed on-call rotations and resolved critical production incidents to ensure pipeline reliability and uptime.

---

## Key Achievements

- Cut S3 storage costs by cleaning up unused data and tuning lifecycle policies — something that hadn't been touched in a while.
- Shipped microservice migrations as part of the org-wide infra overhaul for streaming pipelines.
- Built an MCP server with AI capabilities that made onboarding new BAU tasks way faster than before.
- Revamped the internal portal UI and plugged in Grafana dashboards directly for real-time monitoring.
- Handled on-call duties, triaged incidents, and fixed bugs that were causing job failures.

---

## Impact

- Reduced pipeline onboarding time from ~2 days to a few hours using the MCP tool.
- Helped save infra costs through S3 optimizations (cleanup + better retention policies).
- Kept production stable during on-call shifts by catching and fixing issues early.

---

## Technologies Deep Dive

### Data Engineering
- Scala
- Spark
- Hive
- Hadoop
- Iceberg
- Azkaban
- Zeppelin
- Kafka

### Fullstack
- Python
- ReactJS
- NodeJS
- ExpressJS
- HTML
- CSS
- TailwindCSS
- JavaScript


### Infrastructure
- AWS (S3, ECR, EKS, EC2, EMR, Lambda, RDS)
- Docker
- Kubernetes
- Jenknins
- ArgoCD
- Linux`

### Observability
- Alerting mechanism using emails and slack
- Prometheus & Grafana
