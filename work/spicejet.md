---
id: spicejet-data-engineer
company: "SpiceJet"
role: "Data Engineer"
location: "Bengaluru, IN"
type: "Full-time"
start_date: "2023-01-18"
end_date: "2025-02-18"
is_current: false
company_url: "https://spicejet.com"
logo_url: "https://cdn.brandfetch.io/id0DNBaeFO/theme/dark/logo.svg?c=1dxbfHSJFAPEGdCLU4o5B"
technologies:
  - Python
  - PySpark
  - AWS
  - Microsoft SQL Server
  - Hive
  - Hadoop
  - Iceberg
  - Django
  - HTML
  - CSS/TailwindCSS
description: "Worked on developing near real-time data pipeline along with optimising existing projects"
---

## Overview

At SpiceJet, I worked on big data infrastructure — building a near real-time pipeline from scratch, improving batch processing workflows, and developing internal dashboards for stakeholders. Also worked closely with the DBA team on query monitoring tooling and set up alerting across pipelines.

---

## Key Responsibilities

- Built a near real-time data pipeline from the ground up — pulling data from emails as they arrive, processing with PySpark, and storing in Iceberg tables (Parquet format).
- Maintained and enhanced batch pipelines handling regular BAU tasks across the data platform.
- Developed data visualization dashboards using Django + TailwindCSS to give stakeholders better visibility into business metrics.
- Partnered with the DBA team to build a portal for tracking long-running queries that were hogging resources.
- Set up alerting (email + Slack) to catch pipeline failures early.

---

## Key Achievements

- Shipped the near real-time pipeline end-to-end — from email ingestion to Iceberg storage — enabling analytics that weren't possible before.
- Added pipeline alerting that helped catch failures before they snowballed.
- Built dashboards that stakeholders actually used for day-to-day decision making.
- Refactored batch pipelines into a metadata-driven framework, making onboarding new tasks much simpler.

---

## Impact

- Enabled faster decision-making by giving stakeholders access to near real-time data instead of waiting for batch runs.
- Cut BAU task onboarding time from ~1 week to 2-3 days after switching to the metadata-driven framework.
- Made it easier for DBAs to spot and kill heavy queries before they caused issues.

---

## Technologies Deep Dive

### Data Engineering
- Pyspark
- Hive
- Hadoop
- Iceberg
- MS SQL Server

### Fullstack
- Python
- Django
- HTML
- CSS
- TailwindCSS
- JavaScript


### Infrastructure
- AWS (S3, ECR, EC2, EMR, Step Function, Lambda, Glue, EventBridge, Secret Manager)
- Microsoft IIS (To deploy application)

### Observability
- Alerting mechanism using emails and slack
